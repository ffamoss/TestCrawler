Задание

Обязательные требования
Требуется разработать приложение Web crawler. 
Входные параметры:
1.	Стартовый адрес (строка)
2.	Максимальная глубина обхода (целое)
3.	Требуется ли обходить страницы на доменах, отличных от стартового (да/нет)
4.	Папка для скачивания содержимого (строка)
Приложение должно скачивать содержимое сайта с учетом ограничений (2) и (3), начиная с указанного адреса и сохранять скачанные объекты в указанную локальную папку. 
Требуется обрабатывать следующие типы ссылок: ссылки на ресурсы, картинки, стили css и скрипты.
Для разработки можно использовать любой .NET язык.
Проект должен включать тесты. 
Дополнительные требования, ограничения и предположения, в которых решалась задача, нужно явно сформулировать и приложить к проекту в виде текстового документа.

Пожелания, не обязательные к выполнению
Обновлять ссылки на скачанные объекты на локальные файлы для возможности просмотра сохраненных страниц в оффлайне.  

========================================================
Дополнительный комментарий из переписки

Сергей ,  ребята говорят что ограничений нет, есть только одна ремарка
нужно просто написать код, так как вы бы написали его в жизни на проекте, не надо слишком сильно усложнять

========================================================

Исходя из задания и комментариев явно опушу что осознано не включено в реализацию

1) Общего хранилища файлов для исключения повторного скачивания для разных стартовых адресов
2) Поддержка скачиваний с использованием прокси серверов
3) Не поддерживается авторизация
4) Не учитываются robots.txt
5) Наивная реализаци выделения ссылок из html документа
6) Не реализована политика перезапроса данных при ошибках
7) Замена ссылок работает плохо из-за пункта "5"


Notes:
  Crawler.Core\PageFileSystemStorage.cs(17):     //Note ignore access checks, disk size checks, directory not empty checks...
  Crawler.Core\WebContentExtractor.cs(19):       //Note ignore case when content body has different charset
  Crawler.Core\WebPageLinkManager.cs(11):        //Note naive implementation, many cases not supported

TODO:
  Crawler.Core\DownloaderResult.cs(39):      //TODO copy response parameters 
  Crawler.Console\Program.cs(21):            //TODO input checks